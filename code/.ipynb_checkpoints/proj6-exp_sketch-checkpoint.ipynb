{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/): Sketch_Data extra credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import student_code as sc\n",
    "from torchvision.models import alexnet\n",
    "from torchsummary import summary\n",
    "\n",
    "data_path = osp.join('../data', 'human_sketch')\n",
    "num_classes = 250\n",
    "\n",
    "# If you have a good Nvidia GPU with an appropriate environment, \n",
    "# try setting the use_GPU flag to True (the environment provided does\n",
    "# not support GPUs and we will not provide any support for GPU\n",
    "# computation in this project). Please note that \n",
    "# we will evaluate your implementations only using CPU mode so even if\n",
    "# you use a GPU, make sure your code runs in the CPU mode with the\n",
    "# environment we provided. \n",
    "use_GPU = True\n",
    "if use_GPU:\n",
    "    from utils_gpu import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has two main parts. In Part 1, you will train a deep network from scratch. In Part 2, you will \"fine-tune\" a trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Warm up! Training a Deep Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to code anything for this part. You will simply run the code we provided, but we want you to report the result you got. This section will also familiarize you with the steps of training a deep network from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "input_size = (64, 64)\n",
    "RGB = False  \n",
    "base_lr = 1e-2  # may try a smaller lr if not using batch norm\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create our datasets, by calling the create_datasets function from student_code. This function returns a separate dataset loader for each split of the dataset (training and testing/validation). Each dataloader is used to load the datasets after appling some pre-processing transforms. In Part 1, you will be asked to add a few more pre-processing transforms to the dataloaders by modifying this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 200\n",
      "Batch 20 / 200\n",
      "Batch 40 / 200\n",
      "Batch 60 / 200\n",
      "Batch 80 / 200\n",
      "Batch 100 / 200\n",
      "Batch 120 / 200\n",
      "Batch 140 / 200\n",
      "Batch 160 / 200\n",
      "Batch 180 / 200\n",
      "Done, mean = \n",
      "[0.982125]\n",
      "std = \n",
      "[0.05606342]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 200\n",
      "Batch 20 / 200\n",
      "Batch 40 / 200\n",
      "Batch 60 / 200\n",
      "Batch 80 / 200\n",
      "Batch 100 / 200\n",
      "Batch 120 / 200\n",
      "Batch 140 / 200\n",
      "Batch 160 / 200\n",
      "Batch 180 / 200\n",
      "Done, mean = \n",
      "[0.98213223]\n",
      "std = \n",
      "[0.05596984]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our network model using the SketchNet class from student_code. The implementation provided in the SketchNet class gives you a basic network. In Part 1, you will be asked to add a few more layers to this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SketchNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Conv2d(20, 30, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): ReLU()\n",
      "    (7): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv2d(30, 50, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5)\n",
      "  )\n",
      "  (classifier): Conv2d(50, 250, kernel_size=(8, 8), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the network model.\n",
    "model = sc.SketchNet(num_classes=num_classes, rgb=RGB, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "# summary(model, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function.\n",
    "# see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and a learning rate scheduler\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# Currently a simple step scheduler.\n",
    "# See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "# and how to use them\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to train our network! We will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Experiment: part3\n",
      "experiment: part3\n",
      "n_epochs: 5\n",
      "print_freq: 100\n",
      "resume_optim: True\n",
      "checkpoint_file: None\n",
      "shuffle: True\n",
      "num_workers: 4\n",
      "val_freq: 1\n",
      "batch_size: 100\n",
      "do_val: True\n",
      "---------------------------------------\n",
      "part3 Epoch 0 / 5\n",
      "train part3: batch 0/99, loss 1.786, top-1 accuracy 58.000, top-5 accuracy 79.000\n",
      "train part3: loss 1.539305\n",
      "val part3: batch 0/99, loss 3.027, top-1 accuracy 45.000, top-5 accuracy 60.000\n",
      "val part3: loss 2.470322\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part3 Epoch 1 / 5\n",
      "train part3: batch 0/99, loss 1.494, top-1 accuracy 66.000, top-5 accuracy 87.000\n",
      "train part3: loss 1.463846\n",
      "val part3: batch 0/99, loss 3.080, top-1 accuracy 42.000, top-5 accuracy 61.000\n",
      "val part3: loss 2.473781\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part3 Epoch 2 / 5\n",
      "train part3: batch 0/99, loss 1.685, top-1 accuracy 57.000, top-5 accuracy 85.000\n",
      "train part3: loss 1.379656\n",
      "val part3: batch 0/99, loss 3.111, top-1 accuracy 40.000, top-5 accuracy 59.000\n",
      "val part3: loss 2.483678\n",
      "Checkpoint saved\n",
      "part3 Epoch 3 / 5\n",
      "train part3: batch 0/99, loss 0.845, top-1 accuracy 73.000, top-5 accuracy 96.000\n",
      "train part3: loss 1.295194\n",
      "val part3: batch 0/99, loss 3.090, top-1 accuracy 44.000, top-5 accuracy 59.000\n",
      "val part3: loss 2.486077\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part3 Epoch 4 / 5\n",
      "train part3: batch 0/99, loss 1.033, top-1 accuracy 72.000, top-5 accuracy 93.000\n",
      "train part3: loss 1.256162\n",
      "val part3: batch 0/99, loss 3.343, top-1 accuracy 36.000, top-5 accuracy 59.000\n",
      "val part3: loss 2.557473\n",
      "Checkpoint saved\n",
      "Best top-1 Accuracy = 47.490\n"
     ]
    }
   ],
   "source": [
    "# train the network!\n",
    "params = {'n_epochs': 5, 'batch_size': 100, 'experiment': 'part3'}\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 56, 56]           1,620\n",
      "         MaxPool2d-2           [-1, 20, 27, 27]               0\n",
      "              ReLU-3           [-1, 20, 27, 27]               0\n",
      "       BatchNorm2d-4           [-1, 20, 27, 27]              40\n",
      "            Conv2d-5           [-1, 30, 23, 23]          15,000\n",
      "         MaxPool2d-6           [-1, 30, 11, 11]               0\n",
      "              ReLU-7           [-1, 30, 11, 11]               0\n",
      "       BatchNorm2d-8           [-1, 30, 11, 11]              60\n",
      "            Conv2d-9             [-1, 50, 8, 8]          24,000\n",
      "             ReLU-10             [-1, 50, 8, 8]               0\n",
      "      BatchNorm2d-11             [-1, 50, 8, 8]             100\n",
      "          Dropout-12             [-1, 50, 8, 8]               0\n",
      "           Conv2d-13            [-1, 250, 1, 1]         800,250\n",
      "================================================================\n",
      "Total params: 841,070\n",
      "Trainable params: 841,070\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 1.12\n",
      "Params size (MB): 3.21\n",
      "Estimated Total Size (MB): 4.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary(model, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Using AlexNet to classify sketch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 200\n",
      "Batch 20 / 200\n",
      "Batch 40 / 200\n",
      "Batch 60 / 200\n",
      "Batch 80 / 200\n",
      "Batch 100 / 200\n",
      "Batch 120 / 200\n",
      "Batch 140 / 200\n",
      "Batch 160 / 200\n",
      "Batch 180 / 200\n",
      "Done, mean = \n",
      "[0.98226709 0.98226709 0.98226709]\n",
      "std = \n",
      "[0.09226456 0.09226456 0.09226456]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 200\n",
      "Batch 20 / 200\n",
      "Batch 40 / 200\n",
      "Batch 60 / 200\n",
      "Batch 80 / 200\n",
      "Batch 100 / 200\n",
      "Batch 120 / 200\n",
      "Batch 140 / 200\n",
      "Batch 160 / 200\n",
      "Batch 180 / 200\n",
      "Done, mean = \n",
      "[0.98227521 0.98227521 0.98227521]\n",
      "std = \n",
      "[0.09220306 0.09220306 0.09220306]\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=128, bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=250, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
      "              ReLU-2           [-1, 64, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
      "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
      "              ReLU-5          [-1, 192, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-10          [-1, 256, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "          Dropout-14                 [-1, 9216]               0\n",
      "           Linear-15                 [-1, 4096]      37,752,832\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "           Linear-20                  [-1, 128]         524,288\n",
      "             ReLU-21                  [-1, 128]               0\n",
      "           Linear-22                  [-1, 250]          32,000\n",
      "================================================================\n",
      "Total params: 57,560,128\n",
      "Trainable params: 57,560,128\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 8.30\n",
      "Params size (MB): 219.57\n",
      "Estimated Total Size (MB): 228.45\n",
      "----------------------------------------------------------------\n",
      "---------------------------------------\n",
      "Experiment: part4\n",
      "experiment: part4\n",
      "n_epochs: 4\n",
      "print_freq: 100\n",
      "resume_optim: True\n",
      "checkpoint_file: None\n",
      "shuffle: True\n",
      "num_workers: 4\n",
      "val_freq: 1\n",
      "batch_size: 10\n",
      "do_val: True\n",
      "---------------------------------------\n",
      "part4 Epoch 0 / 4\n",
      "train part4: batch 0/999, loss 5.666, top-1 accuracy 0.000, top-5 accuracy 0.000\n",
      "train part4: batch 100/999, loss 5.376, top-1 accuracy 0.000, top-5 accuracy 0.000\n",
      "train part4: batch 200/999, loss 5.362, top-1 accuracy 10.000, top-5 accuracy 10.000\n",
      "train part4: batch 300/999, loss 5.205, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "train part4: batch 400/999, loss 5.258, top-1 accuracy 10.000, top-5 accuracy 30.000\n",
      "train part4: batch 500/999, loss 5.119, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "train part4: batch 600/999, loss 4.376, top-1 accuracy 10.000, top-5 accuracy 30.000\n",
      "train part4: batch 700/999, loss 4.561, top-1 accuracy 0.000, top-5 accuracy 30.000\n",
      "train part4: batch 800/999, loss 3.970, top-1 accuracy 0.000, top-5 accuracy 30.000\n",
      "train part4: batch 900/999, loss 3.442, top-1 accuracy 20.000, top-5 accuracy 50.000\n",
      "train part4: loss 4.814322\n",
      "val part4: batch 0/999, loss 5.348, top-1 accuracy 0.000, top-5 accuracy 0.000\n",
      "val part4: batch 100/999, loss 3.147, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "val part4: batch 200/999, loss 2.170, top-1 accuracy 60.000, top-5 accuracy 80.000\n",
      "val part4: batch 300/999, loss 1.424, top-1 accuracy 80.000, top-5 accuracy 90.000\n",
      "val part4: batch 400/999, loss 4.239, top-1 accuracy 0.000, top-5 accuracy 20.000\n",
      "val part4: batch 500/999, loss 4.731, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "val part4: batch 600/999, loss 3.246, top-1 accuracy 30.000, top-5 accuracy 70.000\n",
      "val part4: batch 700/999, loss 3.238, top-1 accuracy 50.000, top-5 accuracy 60.000\n",
      "val part4: batch 800/999, loss 4.642, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "val part4: batch 900/999, loss 5.082, top-1 accuracy 10.000, top-5 accuracy 20.000\n",
      "val part4: loss 3.851783\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part4 Epoch 1 / 4\n",
      "train part4: batch 0/999, loss 4.756, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "train part4: batch 100/999, loss 3.407, top-1 accuracy 10.000, top-5 accuracy 40.000\n",
      "train part4: batch 200/999, loss 4.071, top-1 accuracy 10.000, top-5 accuracy 30.000\n",
      "train part4: batch 300/999, loss 3.642, top-1 accuracy 30.000, top-5 accuracy 60.000\n",
      "train part4: batch 400/999, loss 3.941, top-1 accuracy 20.000, top-5 accuracy 40.000\n",
      "train part4: batch 500/999, loss 3.188, top-1 accuracy 30.000, top-5 accuracy 40.000\n",
      "train part4: batch 600/999, loss 3.382, top-1 accuracy 10.000, top-5 accuracy 40.000\n",
      "train part4: batch 700/999, loss 4.054, top-1 accuracy 20.000, top-5 accuracy 30.000\n",
      "train part4: batch 800/999, loss 4.582, top-1 accuracy 0.000, top-5 accuracy 30.000\n",
      "train part4: batch 900/999, loss 3.562, top-1 accuracy 20.000, top-5 accuracy 40.000\n",
      "train part4: loss 3.615892\n",
      "val part4: batch 0/999, loss 3.727, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "val part4: batch 100/999, loss 3.307, top-1 accuracy 20.000, top-5 accuracy 60.000\n",
      "val part4: batch 200/999, loss 2.427, top-1 accuracy 60.000, top-5 accuracy 70.000\n",
      "val part4: batch 300/999, loss 1.063, top-1 accuracy 70.000, top-5 accuracy 90.000\n",
      "val part4: batch 400/999, loss 4.611, top-1 accuracy 0.000, top-5 accuracy 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val part4: batch 500/999, loss 4.581, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "val part4: batch 600/999, loss 3.306, top-1 accuracy 20.000, top-5 accuracy 60.000\n",
      "val part4: batch 700/999, loss 4.362, top-1 accuracy 0.000, top-5 accuracy 0.000\n",
      "val part4: batch 800/999, loss 3.129, top-1 accuracy 50.000, top-5 accuracy 60.000\n",
      "val part4: batch 900/999, loss 4.864, top-1 accuracy 0.000, top-5 accuracy 30.000\n",
      "val part4: loss 3.229136\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part4 Epoch 2 / 4\n",
      "train part4: batch 0/999, loss 2.120, top-1 accuracy 40.000, top-5 accuracy 70.000\n",
      "train part4: batch 100/999, loss 3.571, top-1 accuracy 20.000, top-5 accuracy 30.000\n",
      "train part4: batch 200/999, loss 3.305, top-1 accuracy 30.000, top-5 accuracy 60.000\n",
      "train part4: batch 300/999, loss 3.037, top-1 accuracy 20.000, top-5 accuracy 50.000\n",
      "train part4: batch 400/999, loss 2.354, top-1 accuracy 50.000, top-5 accuracy 60.000\n",
      "train part4: batch 500/999, loss 4.020, top-1 accuracy 10.000, top-5 accuracy 40.000\n",
      "train part4: batch 600/999, loss 2.826, top-1 accuracy 40.000, top-5 accuracy 50.000\n",
      "train part4: batch 700/999, loss 3.129, top-1 accuracy 30.000, top-5 accuracy 60.000\n",
      "train part4: batch 800/999, loss 3.522, top-1 accuracy 10.000, top-5 accuracy 40.000\n",
      "train part4: batch 900/999, loss 3.256, top-1 accuracy 20.000, top-5 accuracy 70.000\n",
      "train part4: loss 3.235334\n",
      "val part4: batch 0/999, loss 4.565, top-1 accuracy 10.000, top-5 accuracy 10.000\n",
      "val part4: batch 100/999, loss 4.063, top-1 accuracy 0.000, top-5 accuracy 40.000\n",
      "val part4: batch 200/999, loss 1.903, top-1 accuracy 70.000, top-5 accuracy 70.000\n",
      "val part4: batch 300/999, loss 1.399, top-1 accuracy 70.000, top-5 accuracy 90.000\n",
      "val part4: batch 400/999, loss 4.896, top-1 accuracy 0.000, top-5 accuracy 10.000\n",
      "val part4: batch 500/999, loss 5.091, top-1 accuracy 0.000, top-5 accuracy 0.000\n",
      "val part4: batch 600/999, loss 2.491, top-1 accuracy 40.000, top-5 accuracy 80.000\n",
      "val part4: batch 700/999, loss 3.325, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "val part4: batch 800/999, loss 1.982, top-1 accuracy 70.000, top-5 accuracy 90.000\n",
      "val part4: batch 900/999, loss 3.343, top-1 accuracy 30.000, top-5 accuracy 60.000\n",
      "val part4: loss 2.988768\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part4 Epoch 3 / 4\n",
      "train part4: batch 0/999, loss 2.601, top-1 accuracy 30.000, top-5 accuracy 80.000\n",
      "train part4: batch 100/999, loss 3.997, top-1 accuracy 20.000, top-5 accuracy 50.000\n",
      "train part4: batch 200/999, loss 2.968, top-1 accuracy 20.000, top-5 accuracy 60.000\n",
      "train part4: batch 300/999, loss 2.744, top-1 accuracy 30.000, top-5 accuracy 70.000\n",
      "train part4: batch 400/999, loss 3.637, top-1 accuracy 20.000, top-5 accuracy 50.000\n",
      "train part4: batch 500/999, loss 1.720, top-1 accuracy 50.000, top-5 accuracy 90.000\n",
      "train part4: batch 600/999, loss 3.075, top-1 accuracy 20.000, top-5 accuracy 70.000\n",
      "train part4: batch 700/999, loss 3.450, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "train part4: batch 800/999, loss 2.742, top-1 accuracy 40.000, top-5 accuracy 60.000\n",
      "train part4: batch 900/999, loss 2.532, top-1 accuracy 30.000, top-5 accuracy 60.000\n",
      "train part4: loss 3.062657\n",
      "val part4: batch 0/999, loss 3.532, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "val part4: batch 100/999, loss 1.940, top-1 accuracy 50.000, top-5 accuracy 80.000\n",
      "val part4: batch 200/999, loss 1.730, top-1 accuracy 60.000, top-5 accuracy 80.000\n",
      "val part4: batch 300/999, loss 1.097, top-1 accuracy 80.000, top-5 accuracy 90.000\n",
      "val part4: batch 400/999, loss 2.192, top-1 accuracy 40.000, top-5 accuracy 70.000\n",
      "val part4: batch 500/999, loss 4.680, top-1 accuracy 0.000, top-5 accuracy 30.000\n",
      "val part4: batch 600/999, loss 1.617, top-1 accuracy 60.000, top-5 accuracy 90.000\n",
      "val part4: batch 700/999, loss 2.764, top-1 accuracy 20.000, top-5 accuracy 60.000\n",
      "val part4: batch 800/999, loss 1.782, top-1 accuracy 90.000, top-5 accuracy 90.000\n",
      "val part4: batch 900/999, loss 3.594, top-1 accuracy 10.000, top-5 accuracy 50.000\n",
      "val part4: loss 2.866972\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "Best top-1 Accuracy = 31.640\n"
     ]
    }
   ],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)\n",
    "\n",
    "# training parameters\n",
    "input_size = (224, 224)\n",
    "RGB = True\n",
    "base_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9\n",
    "backprop_depth = 3\n",
    "\n",
    "\n",
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes\n",
    "\n",
    "# Create the network model.\n",
    "model = alexnet(pretrained=True)\n",
    "print(model)\n",
    "\n",
    "model = sc.create_part2_model(model, num_classes)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "# summary(model, (3, 224, 224))\n",
    "\n",
    "\n",
    "# Set up the trainer. You can modify custom_part2_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part2_trainer = sc.custom_part2_trainer(model)\n",
    "\n",
    "if custom_part2_trainer is None:\n",
    "    # Create the loss function\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Since we do not want to optimize the whole network, we must extract a list of parameters of interest that will be\n",
    "    # optimized by the optimizer.\n",
    "    params_to_optimize = []\n",
    "\n",
    "    # List of modules in the network\n",
    "    mods = list(model.features.children()) + list(model.classifier.children())\n",
    "\n",
    "    # Extract parameters from the last `backprop_depth` modules in the network and collect them in\n",
    "    # the params_to_optimize list.\n",
    "    for m in mods[::-1][:backprop_depth]:\n",
    "        params_to_optimize.extend(list(m.parameters()))\n",
    "\n",
    "    # Construct the optimizer    \n",
    "    optimizer = optim.SGD(params=params_to_optimize, lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    # Create a scheduler, currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    params = {'n_epochs': 4, 'batch_size': 10, 'experiment': 'part4'} \n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part2_trainer:\n",
    "        loss_function = custom_part2_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part2_trainer:\n",
    "        optimizer = custom_part2_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part2_trainer:\n",
    "        lr_scheduler = custom_part2_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part2_trainer:\n",
    "        params = custom_part2_trainer['params']\n",
    "        \n",
    "        \n",
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
